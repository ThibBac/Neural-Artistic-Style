{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8d800d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44b4c2f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vgg_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3), pooling='avg')\n",
    "# vgg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53b99b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_response(image, model, layers_name=None):\n",
    "    \n",
    "    if layers_name is None:\n",
    "        layers_name = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n",
    "    \n",
    "        \n",
    "    filter_dict = {}\n",
    "    outputs = [layer.output for layer in model.layers if layer.name in layers_name]\n",
    "    model = Model(inputs=model.input, outputs=outputs)\n",
    "    \n",
    "    feature_maps = model.predict(image)\n",
    "    for layer, filters in zip(layers_name, feature_maps):\n",
    "        filter_dict[layer] = filters\n",
    "            \n",
    "    return filter_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6fd4cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path, preprocess=False):\n",
    "    \n",
    "    img = image.load_img(path, target_size=(224, 224))\n",
    "    img = image.img_to_array(img)\n",
    "    if preprocess:\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        img = preprocess_input(img)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccce5711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(content, combine):\n",
    "    squared_difference = tf.square(content - combine)\n",
    "    \n",
    "    return tf.reduce_mean(squared_difference, axis=-1)\n",
    "\n",
    "def gram_matrix(feature_map):\n",
    "    height, width, depth = feature_map.shape\n",
    "\n",
    "    feature_map = tf.reshape(feature_map, (depth, height * width))\n",
    "    gram_matrix = tf.matmul(feature_map, tf.transpose(feature_map))\n",
    "    \n",
    "    return gram_matrix\n",
    "\n",
    "def style_loss(style, combination):\n",
    "    \n",
    "    width, height, filters = style.shape\n",
    "    \n",
    "    style = gram_matrix(style)\n",
    "    combination = gram_matrix(combination)\n",
    "    \n",
    "    layer_loss = tf.square(style - combination)\n",
    "    \n",
    "    return 1 / (4 * (width * height) * filters**2 * tf.reduce_mean(layer_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1cdad7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = tf.Variable(load_image('./styles/kitten.jpg', preprocess=True))\n",
    "style_image = tf.Variable(load_image('./styles/starry_night.jpg', preprocess=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7588763f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 136ms/step\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002F113E7D790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 133ms/step\n"
     ]
    }
   ],
   "source": [
    "style_features = features_response(style, vgg_model)\n",
    "content_features = features_response(content, vgg_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "be6893e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combination_image = np.random.randint(0, 255, (content.shape[1:])).astype(np.uint8)\n",
    "combination_image = tf.Variable(np.expand_dims(combination_image, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1ef7bacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(content, combination):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        pred = vgg_model(X)\n",
    "        \n",
    "        y = content_features['block5_conv1']\n",
    "        loss = content_loss(y, content)\n",
    "\n",
    "    grads = tape.gradient(loss, content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ae92c92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] starting epoch 1/25..."
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer \"block1_conv1\" (type Conv2D).\n\nValue for attr 'T' of uint8 is not in the list of allowed values: half, bfloat16, float, double, int32\n\t; NodeDef: {{node Conv2D}}; Op<name=Conv2D; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=[\"SAME\", \"VALID\", \"EXPLICIT\"]; attr=explicit_paddings:list(int),default=[]; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]; attr=dilations:list(int),default=[1, 1, 1, 1]> [Op:Conv2D]\n\nCall arguments received by layer \"block1_conv1\" (type Conv2D):\n  • inputs=<tf.Variable 'Variable:0' shape=(1, 224, 224, 3) dtype=uint8, numpy=\narray([[[[100, 222,  86],\n         [110,  83, 109],\n         [124,  59,  35],\n         ...,\n         [146, 166, 176],\n         [163, 202, 220],\n         [  8,  79, 137]],\n\n        [[ 48, 237, 191],\n         [211, 119, 213],\n         [  2,  47, 158],\n         ...,\n         [ 68, 227, 176],\n         [204, 156, 202],\n         [ 48, 228, 201]],\n\n        [[182,  84,  36],\n         [ 60,  95,  55],\n         [158, 174, 190],\n         ...,\n         [157,  22, 122],\n         [  9,  15,  61],\n         [175, 118, 158]],\n\n        ...,\n\n        [[ 55, 193,  83],\n         [ 86,  65, 241],\n         [121, 237, 248],\n         ...,\n         [156, 204,  79],\n         [140,  85, 170],\n         [ 23,  12,  98]],\n\n        [[150, 133,  42],\n         [160, 195, 115],\n         [126,  72, 142],\n         ...,\n         [ 84, 183, 182],\n         [ 86, 178, 175],\n         [233, 190,  64]],\n\n        [[ 73, 189, 242],\n         [136,  12,  83],\n         [253, 109, 231],\n         ...,\n         [ 81,  66, 184],\n         [155, 113, 158],\n         [ 35, 195,  48]]]], dtype=uint8)>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[1;32mIn [47]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m epochStart \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# loop over the data in batch size increments\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# take a step\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# show timing information for the epoch\u001b[39;00m\n\u001b[0;32m     17\u001b[0m epochEnd \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Input \u001b[1;32mIn [46]\u001b[0m, in \u001b[0;36mstep\u001b[1;34m(X, y)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(X, y):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# keep track of our gradients\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;66;03m# make a prediction using the model and then calculate the\u001b[39;00m\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;66;03m# loss\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m         pred \u001b[38;5;241m=\u001b[39m \u001b[43mvgg_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m         y \u001b[38;5;241m=\u001b[39m content_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblock5_conv1\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      9\u001b[0m         loss \u001b[38;5;241m=\u001b[39m content_loss(y, content)\n",
      "File \u001b[1;32mc:\\users\\thib\\pycharmprojects\\neural-artistic-style\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\users\\thib\\pycharmprojects\\neural-artistic-style\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:7164\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7163\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 7164\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer \"block1_conv1\" (type Conv2D).\n\nValue for attr 'T' of uint8 is not in the list of allowed values: half, bfloat16, float, double, int32\n\t; NodeDef: {{node Conv2D}}; Op<name=Conv2D; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=[\"SAME\", \"VALID\", \"EXPLICIT\"]; attr=explicit_paddings:list(int),default=[]; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]; attr=dilations:list(int),default=[1, 1, 1, 1]> [Op:Conv2D]\n\nCall arguments received by layer \"block1_conv1\" (type Conv2D):\n  • inputs=<tf.Variable 'Variable:0' shape=(1, 224, 224, 3) dtype=uint8, numpy=\narray([[[[100, 222,  86],\n         [110,  83, 109],\n         [124,  59,  35],\n         ...,\n         [146, 166, 176],\n         [163, 202, 220],\n         [  8,  79, 137]],\n\n        [[ 48, 237, 191],\n         [211, 119, 213],\n         [  2,  47, 158],\n         ...,\n         [ 68, 227, 176],\n         [204, 156, 202],\n         [ 48, 228, 201]],\n\n        [[182,  84,  36],\n         [ 60,  95,  55],\n         [158, 174, 190],\n         ...,\n         [157,  22, 122],\n         [  9,  15,  61],\n         [175, 118, 158]],\n\n        ...,\n\n        [[ 55, 193,  83],\n         [ 86,  65, 241],\n         [121, 237, 248],\n         ...,\n         [156, 204,  79],\n         [140,  85, 170],\n         [ 23,  12,  98]],\n\n        [[150, 133,  42],\n         [160, 195, 115],\n         [126,  72, 142],\n         ...,\n         [ 84, 183, 182],\n         [ 86, 178, 175],\n         [233, 190,  64]],\n\n        [[ 73, 189, 242],\n         [136,  12,  83],\n         [253, 109, 231],\n         ...,\n         [ 81,  66, 184],\n         [155, 113, 158],\n         [ 35, 195,  48]]]], dtype=uint8)>"
     ]
    }
   ],
   "source": [
    "EPOCHS = 25\n",
    "BS = 64\n",
    "INIT_LR = 1e-3\n",
    "\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "\n",
    "for epoch in range(0, EPOCHS):\n",
    "    \n",
    "    print(\"[INFO] starting epoch {}/{}...\".format(epoch + 1, EPOCHS), end=\"\")\n",
    "    sys.stdout.flush()\n",
    "    epochStart = time.time()\n",
    "\n",
    "    loss, grads = step(x, content)\n",
    "    opt.apply_gradients(zip(grads, content))\n",
    "    \n",
    "    \n",
    "    epochEnd = time.time()\n",
    "    elapsed = (epochEnd - epochStart) / 60.0\n",
    "    print(\"took {:.4} minutes\".format(elapsed))\n",
    "\n",
    "vgg_model.compile(optimizer=opt, loss=content_loss, metrics=[\"MSE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2271d1aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 145ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16876285",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
